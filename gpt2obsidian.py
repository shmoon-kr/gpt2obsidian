#!/usr/bin/env python3
# gpt2obsidian_simple.py
# ëª©ì : ChatGPT ë‚´ë³´ë‚´ê¸° JSON â†’ "ëŒ€í™”ë³„ ì‘ì€ md íŒŒì¼"ë¡œ ìª¼ê°œì–´ Obsidianì— ë„£ê¸°
#      ê·¸ ì™¸ íŒŒì¼ì€ ì²¨ë¶€ í´ë”ë¡œ ì´ë™. ê±°ëŒ€ í•©ë³¸ íŒŒì¼ ì ˆëŒ€ ìƒì„± ì•ˆ í•¨.
# ex) python .\gpt2obsidian.py --src "E:\OneDrive\ChatbotGame\GPT\Threads\20250815" --prefix-date --copy --subfolder "Archive\\ChatGPT_20250815" --attachments "_attachments\\"
# ap.add_argument("--vault", default="G:\\ë‚´ ë“œë¼ì´ë¸Œ\\Obsidian\\DarkLord\\", help="Obsidian Vault ë£¨íŠ¸ ê²½ë¡œ")

#!/usr/bin/env python3
# gpt2obsidian.py
# ëª©ì : ChatGPT ë‚´ë³´ë‚´ê¸° JSON â†’ "ëŒ€í™”ë³„ ì‘ì€ md íŒŒì¼"ë¡œ ë¶„í•  ì €ì¥ (í•©ë³¸ ìƒì„± ì—†ìŒ)
#      + (UPD) ìš”ì•½ì€ ë³„ë„ íŒŒì¼ì„ ë§Œë“¤ì§€ ì•Šê³  í”„ëŸ°íŠ¸ë§¤í„° summaryì—ë§Œ ì €ì¥
#      + (NEW) ìë™ íƒœê·¸/ìˆ˜ë™ íƒœê·¸ ì§€ì›
#
# ì˜ˆ)
#  python .\gpt2obsidian.py --src "E:\export" --vault "D:\Vault" \
#    --subfolder "ChatGPT" --attachments "_attachments" --prefix-date --copy \
#    --summarize --model gpt-4o-mini --auto-tags --tags "chatgpt,work"

import argparse
import json
import os
import re
import shutil
import sys
from datetime import datetime, timezone
from pathlib import Path
from typing import Optional, List

# ---- (ì„ íƒ) .env ìë™ ë¡œë“œ ----
try:
    from dotenv import load_dotenv, find_dotenv
    load_dotenv(find_dotenv(usecwd=True) or ".env")
except Exception:
    pass

ATTACHMENT_EXTS = {
    ".png", ".jpg", ".jpeg", ".gif", ".webp", ".svg",
    ".pdf", ".mp3", ".wav", ".m4a", ".mp4", ".mov", ".webm",
    ".csv", ".pptx", ".xlsx", ".docx", ".zip", ".heic"
}
TEXTLIKE_EXTS = {".md", ".txt"}

def now_iso():
    return datetime.now().astimezone().isoformat(timespec="seconds")

def iso_from_epoch(ts):
    try:
        return datetime.fromtimestamp(float(ts), tz=timezone.utc).astimezone().isoformat(timespec="seconds")
    except Exception:
        return now_iso()

def slugify(name: str) -> str:
    base = Path(name).stem if "." in name else name
    base = base.strip()
    base = re.sub(r"[\s/]+", "-", base)
    base = re.sub(r"[^\w\-\u3131-\u318E\uAC00-\uD7A3]", "", base)  # í•œê¸€ í—ˆìš©
    base = re.sub(r"-{2,}", "-", base).strip("-_")
    return base or "note"

def ensure_unique(path: Path) -> Path:
    if not path.exists():
        return path
    stem, suf = path.stem, path.suffix
    i = 2
    while True:
        cand = path.with_name(f"{stem}-{i}{suf}")
        if not cand.exists():
            return cand
        i += 1

def build_fm_dict(title, original_filename=None, extra=None):
    fm = {"title": title, "created": now_iso(), "source": "ChatGPT"}
    if original_filename: fm["original_filename"] = original_filename
    if extra: fm.update(extra)
    return fm

def dump_frontmatter(fm: dict) -> str:
    lines = ["---"]
    for k, v in fm.items():
        if isinstance(v, list):
            # íƒœê·¸/ë¦¬ìŠ¤íŠ¸
            safe_items = [str(x).replace("\n"," ").strip() for x in v]
            lines.append(f"{k}: [{', '.join(safe_items)}]")
        else:
            v_str = str(v).replace("\n", "\\n")
            lines.append(f"{k}: {v_str}")
    lines.append("---\n")
    return "\n".join(lines)

def content_to_text_any(content):
    if content is None:
        return ""
    if isinstance(content, str):
        return content
    if isinstance(content, dict):
        if "text" in content and isinstance(content["text"], str):
            return content["text"]
        if "parts" in content and isinstance(content["parts"], list):
            return "\n".join(str(p) for p in content["parts"])
        if "content" in content and isinstance(content["content"], list):
            parts = []
            for it in content["content"]:
                if isinstance(it, dict) and it.get("type") == "text":
                    parts.append(it.get("text",""))
            return "\n".join(parts)
    if isinstance(content, list):
        parts = []
        for it in content:
            if isinstance(it, dict) and it.get("type") == "text":
                parts.append(it.get("text",""))
            else:
                parts.append(str(it))
        return "\n".join(parts)
    return str(content)

def role_of(msg):
    role = msg.get("role")
    if not role and isinstance(msg.get("author"), dict):
        role = msg["author"].get("role")
    if not role and isinstance(msg.get("message"), dict):
        role = (msg["message"].get("author") or {}).get("role")
    if not role and "from" in msg:
        role = msg["from"]
    return role or "unknown"

def text_of(msg):
    if "content" in msg:
        return content_to_text_any(msg["content"]).strip()
    if isinstance(msg.get("message"), dict):
        return content_to_text_any(msg["message"].get("content")).strip()
    if "value" in msg and isinstance(msg["value"], str):
        return msg["value"].strip()
    if "choices" in msg and isinstance(msg["choices"], list) and msg["choices"]:
        ch = msg["choices"][0]
        if isinstance(ch, dict) and "message" in ch:
            return (ch["message"].get("content") or "").strip()
    return ""

def time_of(msg):
    for k in ("create_time","created","timestamp","created_at"):
        if k in msg and msg[k] is not None:
            return iso_from_epoch(msg[k])
    if isinstance(msg.get("message"), dict) and msg["message"].get("create_time") is not None:
        return iso_from_epoch(msg["message"]["create_time"])
    return now_iso()

def normalize_messages(seq):
    out = []
    for m in seq:
        t = text_of(m)
        out.append({"role": role_of(m), "text": t if t else "(no content)", "ts": time_of(m)})
    return out

def collect_from_mapping(mapping: dict):
    nodes = []
    for node in mapping.values():
        msg = node.get("message")
        if msg: nodes.append(msg)
    def key(m):
        ct = m.get("create_time")
        return (0 if ct is None else 1, ct or 0.0)
    nodes.sort(key=key)
    return normalize_messages(nodes)

def parse_chat_items(data):
    """dict or list â†’ [{title, created, messages:[...]}, ...]"""
    out = []
    if isinstance(data, list):
        for item in data:
            if not isinstance(item, dict): continue
            title = item.get("title") or "chat"
            created = iso_from_epoch(item.get("create_time")) if item.get("create_time") else now_iso()
            if isinstance(item.get("mapping"), dict):
                msgs = collect_from_mapping(item["mapping"])
            elif isinstance(item.get("messages"), list):
                msgs = normalize_messages(item["messages"])
            else:
                msgs = []
            if msgs:
                out.append({"title": title, "created": created, "messages": msgs})
        return out

    if not isinstance(data, dict):
        return out

    if isinstance(data.get("messages"), list):
        out.append({"title": data.get("title") or "chat",
                    "created": iso_from_epoch(data.get("create_time")) if data.get("create_time") else now_iso(),
                    "messages": normalize_messages(data["messages"])})
        return out

    if isinstance(data.get("mapping"), dict):
        out.append({"title": data.get("title") or "chat",
                    "created": iso_from_epoch(data.get("create_time")) if data.get("create_time") else now_iso(),
                    "messages": collect_from_mapping(data["mapping"])})
        return out

    if isinstance(data.get("conversations"), list):
        for conv in data["conversations"]:
            title = conv.get("title") or "chat"
            created = iso_from_epoch(conv.get("create_time")) if conv.get("create_time") else now_iso()
            if isinstance(conv.get("mapping"), dict):
                msgs = collect_from_mapping(conv["mapping"])
            elif isinstance(conv.get("messages"), list):
                msgs = normalize_messages(conv["messages"])
            else:
                msgs = []
            if msgs:
                out.append({"title": title, "created": created, "messages": msgs})
        return out

    return out

def md_from_conv(conv):
    lines = [f"# {conv.get('title','chat')}\n"]
    for m in conv["messages"]:
        who = {"system":"âš™ï¸ system","user":"ğŸ§‘ user","assistant":"ğŸ¤– assistant"}.get(m["role"], m["role"])
        lines.append(f"### {who} â€” {m['ts']}")
        lines.append(m["text"] if m["text"] else "_(no content)_")
        lines.append("")
    return "\n".join(lines).strip() + "\n"

# --------- ìš”ì•½(ì œëª©/ë³¸ë¬¸) ìƒì„± ---------

def _openai_chat_complete(prompt: str, model: str = "gpt-4o-mini", max_tokens: int = 256) -> Optional[str]:
    api_key = os.environ.get("OPENAI_API_KEY")
    if not api_key:
        return None
    import requests, time as _t
    url = "https://api.openai.com/v1/chat/completions"
    headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}
    payload = {
        "model": model,
        "messages": [
            {"role": "system", "content": "You are a concise Korean writing assistant."},
            {"role": "user", "content": prompt}
        ],
        "temperature": 0.2,
        "top_p": 0.9,
        "max_tokens": max_tokens
    }
    for attempt in range(3):
        try:
            r = requests.post(url, headers=headers, json=payload, timeout=60)
            if r.status_code == 200:
                return r.json()["choices"][0]["message"]["content"].strip()
            if r.status_code in (429, 500, 502, 503):
                _t.sleep(1.2 * (attempt + 1)); continue
            break
        except Exception:
            _t.sleep(1.2 * (attempt + 1))
    return None

def summarize_block_kor(text: str, model: str) -> tuple[str, str]:
    MAX = 14000
    text4 = text[:9000] + "\n...\n" + text[-4000:] if len(text) > MAX else text

    title_prompt = "ë‹¤ìŒ í•œêµ­ì–´ ë¬¸ì„œì˜ í•µì‹¬ì„ 1ì¤„ ì œëª©(35ì ì´ë‚´)ìœ¼ë¡œ ì‘ì„±:\n\n" + text4
    body_prompt  = "ë‹¤ìŒ í•œêµ­ì–´ ë¬¸ì„œë¥¼ 5~8ì¤„ë¡œ ê°„ê²°íˆ ìš”ì•½(ë¶ˆë¦¿X, ë‹¨ë½í˜•):\n\n" + text4

    title = _openai_chat_complete(title_prompt, model=model, max_tokens=64)
    summ  = _openai_chat_complete(body_prompt,  model=model, max_tokens=360)

    if title and summ:
        return title.strip(), summ.strip()

    # --- ë¡œì»¬ í´ë°± ---
    first_line = next((ln.strip() for ln in text.splitlines() if ln.strip()), "ëŒ€í™” ìš”ì•½")
    if len(first_line) > 30: first_line = first_line[:30] + "â€¦"
    parts = re.split(r"(?<=[\.!?ã€‚â€¦])\s+", text.strip())
    if len(parts) >= 6:
        summ_local = " ".join(parts[:6])
    else:
        summ_local = text[:600]
    return first_line, summ_local.strip()

# --------- íƒœê·¸ ì¶”ë¡  ---------

KEYWORD_TAGS = [
    (r"\bMTG\b|ë§¤ì§ë”ê°œë”ë§|ë§¤ì§ ë” ê°œë”ë§", "mtg"),
    (r"ë¬´ìë¹„ì˜ ê²½ê¸°ì¥", "ë¬´ìë¹„ì˜ê²½ê¸°ì¥"),
    (r"\bë¦´\b|ë¦´ë¦¬ìŠ¤ ë…¹íƒ€", "ë¦´"),
    (r"\bAWS\b|EC2|S3|ELB", "aws"),
    (r"\bObsidian\b|ì˜µì‹œë””ì–¸", "obsidian"),
    (r"\bCustom GPT\b|ì»¤ìŠ¤í…€ GPT|í”„ë¡¬í”„íŠ¸", "prompt"),
    (r"ë¼ë…¸ë²¨|ì›¹ì†Œì„¤", "novel"),
    (r"\bAR\b|\bVR\b", "xr"),
]

def infer_tags(text: str, base: List[str]) -> List[str]:
    tags = set(x.strip() for x in base if x.strip())
    for pat, tag in KEYWORD_TAGS:
        if re.search(pat, text, flags=re.IGNORECASE):
            tags.add(tag)
    # í•­ìƒ ë“¤ì–´ê°ˆ ê¸°ë³¸ íƒœê·¸
    tags.add("chatgpt")
    return sorted(tags)

# --------- ë¶„í•  ---------

def split_text_chunks(text: str, max_chars: int):
    if max_chars is None:
        return [text]
    if len(text) <= max_chars:
        return [text]
    chunks = []
    paras = text.split("\n\n")
    cur = ""
    for p in paras:
        add = (p + "\n\n")
        if len(cur) + len(add) > max_chars and cur:
            chunks.append(cur); cur = add
        else:
            cur += add
        if len(cur) >= int(max_chars * 1.05):
            chunks.append(cur); cur = ""
    if cur: chunks.append(cur)
    final = []
    for c in chunks:
        if len(c) <= max_chars:
            final.append(c)
        else:
            for j in range(0, len(c), max_chars):
                final.append(c[j:j+max_chars])
    return final

def get_chunks(text: str, max_chars):
    return split_text_chunks(text, max_chars)

# --------- ë©”ì¸ íŒŒì´í”„ë¼ì¸ ---------

def main():
    ap = argparse.ArgumentParser(description="ChatGPT JSON â†’ ê°œë³„ ì‘ì€ MD, ì²¨ë¶€ ì´ë™(ì‹¬í”Œ) + (ì˜µì…˜) ìš”ì•½/íƒœê·¸")
    ap.add_argument("--src", required=True, help="ì†ŒìŠ¤ í´ë” (ChatGPT ë‚´ë³´ë‚´ê¸° JSON/ì²¨ë¶€)")
    ap.add_argument("--vault", default="G:\\ë‚´ ë“œë¼ì´ë¸Œ\\Obsidian\\DarkLord\\", help="Obsidian Vault ë£¨íŠ¸")
    ap.add_argument("--subfolder", default="ChatGPT", help="ë…¸íŠ¸ ì €ì¥ í•˜ìœ„ í´ë”")
    ap.add_argument("--attachments", default="_attachments", help="ì²¨ë¶€ ì €ì¥ í´ë”ëª…")
    ap.add_argument("--copy", action="store_true", help="ì´ë™ ëŒ€ì‹  ë³µì‚¬")
    ap.add_argument("--prefix-date", action="store_true", help="íŒŒì¼ëª… ì•ì— YYYY-MM-DD_")
    ap.add_argument("--glob", default="*.json", help="ì²˜ë¦¬í•  JSON íŒ¨í„´ (ê¸°ë³¸: *.json)")
    ap.add_argument("--chunk-chars", type=int, default=None, help="ë…¸íŠ¸ ìµœëŒ€ ê¸€ì ìˆ˜(ì—†ìœ¼ë©´ ë¶„í•  ì—†ìŒ)")
    ap.add_argument("--embed-for-attachments", action="store_true", help="ì²¨ë¶€ ì„ë² ë“œ ë…¸íŠ¸ ìƒì„±")

    # ìš”ì•½/íƒœê·¸
    ap.add_argument("--summarize", action="store_true", help="OpenAI ì‚¬ìš©í•´ ê° ë…¸íŠ¸ ìš”ì•½ ìƒì„±(í”„ëŸ°íŠ¸ë§¤í„° summaryì— ì €ì¥)")
    ap.add_argument("--model", default="gpt-4o-mini", help="ìš”ì•½ìš© OpenAI ëª¨ë¸")
    ap.add_argument("--auto-tags", action="store_true", help="ë³¸ë¬¸ í‚¤ì›Œë“œ ê¸°ë°˜ ìë™ íƒœê·¸ ì¶”ê°€")
    ap.add_argument("--tags", default="", help="ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ìˆ˜ë™ íƒœê·¸ (ì˜ˆ: 'work,diary')")

    args = ap.parse_args()

    src_dir = Path(os.path.expanduser(args.src)).resolve()
    vault_dir = Path(os.path.expanduser(args.vault)).resolve()
    notes_dir = vault_dir / args.subfolder
    attach_dir = vault_dir / args.attachments

    if not src_dir.exists():
        print(f"[ì—ëŸ¬] ì†ŒìŠ¤ í´ë” ì—†ìŒ: {src_dir}", file=sys.stderr); sys.exit(1)
    if not vault_dir.exists():
        print(f"[ì—ëŸ¬] Vault í´ë” ì—†ìŒ: {vault_dir}", file=sys.stderr); sys.exit(1)

    notes_dir.mkdir(parents=True, exist_ok=True)
    attach_dir.mkdir(parents=True, exist_ok=True)

    date_prefix = datetime.now().strftime("%Y-%m-%d_") if args.prefix_date else ""

    manual_tags = [t.strip() for t in args.tags.split(",")] if args.tags else []

    # 1) JSON ì²˜ë¦¬
    json_files = list(src_dir.glob(args.glob))
    processed = 0
    for jf in json_files:
        try:
            raw = jf.read_text(encoding="utf-8")
            data = json.loads(raw)
        except Exception as e:
            print(f"[ê²½ê³ ] JSON íŒŒì‹± ì‹¤íŒ¨: {jf.name} ({e})")
            continue

        convs = parse_chat_items(data)
        if not convs:
            # êµ¬ì¡° ë¯¸ì¸ì‹ â†’ ì›ë¬¸ì„ ì½”ë“œë¸”ë¡ìœ¼ë¡œë¼ë„ ë³´ì¡´
            title = slugify(jf.stem)
            body = "```json\n" + raw + "\n```\n"
            chunks = get_chunks(body, args.chunk_chars)
            for idx, ch in enumerate(chunks, 1):
                name = f"{date_prefix}_{idx:03d}_{title}.md"
                target = ensure_unique(notes_dir / name)

                # í”„ëŸ°íŠ¸ë§¤í„° êµ¬ì„±
                extra = {"original_filename": jf.name}
                fm = build_fm_dict(title, jf.name, extra)

                # ìš”ì•½/íƒœê·¸
                if args.summarize:
                    _, summary = summarize_block_kor(ch, model=args.model)
                    fm["summary"] = summary
                if args.auto_tags:
                    fm["tags"] = infer_tags(ch, manual_tags)
                elif manual_tags:
                    fm["tags"] = manual_tags

                target.write_text(dump_frontmatter(fm) + ch, encoding="utf-8")
                print(f"[ëŒ€í™”ì›ë¬¸] {jf.name} â†’ {target.relative_to(vault_dir)}")
                processed += 1
            # ì›ë³¸ ì •ë¦¬
            if not args.copy:
                jf.unlink(missing_ok=True)
            continue

        # ì •ê·œ ëŒ€í™” ì¼€ì´ìŠ¤
        for ci, conv in enumerate(convs, 1):
            base_title = slugify(conv["title"]) or slugify(jf.stem)
            md_full = md_from_conv(conv)
            parts = get_chunks(md_full, args.chunk_chars)

            for si, ch in enumerate(parts, 1):
                name = f"{date_prefix}"
                if len(convs) > 1: name += f"_{ci:03d}"
                if len(parts) > 1: name += f"_part{si:02d}"
                name += f"_{base_title}.md"

                target = ensure_unique(notes_dir / name)
                extra = {"created": conv.get("created", now_iso()), "original_filename": jf.name}
                fm = build_fm_dict(conv["title"], jf.name, extra)

                # ìš”ì•½/íƒœê·¸
                if args.summarize:
                    _, summary = summarize_block_kor(ch, model=args.model)
                    fm["summary"] = summary
                if args.auto_tags:
                    fm["tags"] = infer_tags(ch, manual_tags)
                elif manual_tags:
                    fm["tags"] = manual_tags

                target.write_text(dump_frontmatter(fm) + ch, encoding="utf-8")
                print(f"[ëŒ€í™”ë…¸íŠ¸] {jf.name} â†’ {target.relative_to(vault_dir)}")
                processed += 1

        if not args.copy:
            jf.unlink(missing_ok=True)

    # 2) ì²¨ë¶€/ê¸°íƒ€ íŒŒì¼ ì´ë™
    for src in src_dir.iterdir():
        if src.is_dir(): continue
        ext = src.suffix.lower()
        if ext == ".json":
            continue
        if ext in ATTACHMENT_EXTS or ext not in TEXTLIKE_EXTS:
            dst = ensure_unique(attach_dir / f"{date_prefix}{src.name}")
            if args.copy: shutil.copy2(src, dst)
            else: shutil.move(src, dst)
            print(f"[ì²¨ë¶€] {src.name} â†’ {dst.relative_to(vault_dir)}")
            if args.embed_for_attachments:
                stub = ensure_unique(notes_dir / f"{date_prefix}{slugify(src.stem)}.md")
                fm = build_fm_dict(slugify(src.stem), src.name, {"tags":["chatgpt","attachment"]})
                rel = str(dst.relative_to(vault_dir)).replace("\\","/")
                stub.write_text(dump_frontmatter(fm) + f"![[{rel}]]\n", encoding="utf-8")
                print(f"[ì„ë² ë“œë…¸íŠ¸] {stub.relative_to(vault_dir)}")
                processed += 1
        else:
            pass

    print(f"\nì™„ë£Œ. ìƒì„±/ì´ë™ëœ í•­ëª©: {processed}")

if __name__ == "__main__":
    main()
